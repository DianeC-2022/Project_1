import org.apache.spark.sql.SparkSession
import java.sql.Connection
import scala.io.StdIn.{readInt, readLine}

//object Scenario {


    // create a spark session
    // for Windows
    //System.setProperty("hadoop.home.dir", "C:\\hadoop")

   // val spark = SparkSession.builder()
      //.appName("HiveTest5")
     // .config("spark.master", "local")
      //.enableHiveSupport()
      //.getOrCreate()
   // spark.sparkContext.setLogLevel("ERROR")


        // println("created spark session")

        // def menu(): Unit = 6

        //def Scenario1(con: Connection): Unit = {

        //val Scenario = readInt()

        //def displayJobs(con: Connection): Unit = {
            //val statement = con.createStatement
            //val data = readInt()
            //"""
              //|Main Menu
              //|1. Scenario 1
              //|2. Scenario 2
              //|3. Scenario 3
              //|4. Scenario 4
              //|5. Scenario 5
              //|6. Scenario 6
              //|7. Quit
              //|""".stripMargin;

            //val menuSelection = readInt()
           // menuSelection match {

                //case 1
               // => Scenario1(con)
                //menu()
                //case 2
                //=> Scenario2(con)
               // menu()
                //case 3
                //=> Scenario3(con)
                //menu()
                //case 4
                //=> Scenario4(con)
                //menu()
                //case 5
                //=> Scenario5(con)
                //menu()
                //case 6
                //=> Scenario6(con)
                //menu()
            //}
        //}
    //}


//}
